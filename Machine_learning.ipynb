{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lenna888/Classifier_ML_Dsmatallana_Lelatorre_802/blob/main/Machine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clasificador de correo SPAM - HAM**\n",
        "---\n",
        "Este script tiene como objetivo construir un modelo de Regresión Logística para la clasificación de correos SPAM y HAM. Se realiza un análisis de las características (features) para el entrenamiento del modelo. Además se usará F1 Score y matrices de confusión como evaluador del modelo.\n",
        "\n",
        "\n",
        "@authors: Duvan Santiago Matallana Jiménez - Laura Estefania Latorre Pachon Grupo: 802  \n",
        "@date: 2025-09-10\n"
      ],
      "metadata": {
        "id": "ne0L7yV_DVU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EF3lIxE5Ek03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Importación de librerias**"
      ],
      "metadata": {
        "id": "zfHGlv5sUtyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "print(\"Successful imports\")"
      ],
      "metadata": {
        "id": "n4siSHYERju4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lectura y normalización de los features dominio_coincide y dominio_remitente (Cambio de texto a numérico)"
      ],
      "metadata": {
        "id": "C7icO4JtgruG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  #Load the dataset\n",
        "  df = pd.read_csv('/content/drive/MyDrive/Machine_learning/Dataset_linear_model/dataset_con_clase.csv', sep=';')\n",
        "  print(\"Dataset has been upload correctly, watch to aprove:\")\n",
        "  print(df.head())\n",
        "except FileNotFoundError:\n",
        "  print(\"Error: file not exist or wrong directory\")\n",
        "  df = None\n",
        "\n",
        "if df is not None:\n",
        "    # Normalize sender_domain vs reply_domain\n",
        "    # Create a new column ‘domain_matches’\n",
        "    # It will be 1 if the domains are the same, 0 if they are different.\n",
        "    df['dominio_coincide'] = (df['dominio_remitente'] == df['dominio_respuesta']).astype(int)\n",
        "    print(\"\\nThe column ‘dominio_coincide’ has been created.\")\n",
        "    df = df.drop(columns=['dominio_remitente', 'dominio_respuesta'])\n",
        "    print(\"\\nThe columns ‘dominio_remitente’ and ‘dominio_respuesta’ have been removed.\")\n",
        "\n",
        "    X = df.drop('clase', axis=1)\n",
        "    y = df['clase']\n",
        "    print(\"\\nThe variables X and y have been created.\")\n",
        "    print(X.columns.tolist())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9uR8KPnkTwVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se revisan las **correlaciones** entre las features para empezar con las diferentes pruebas y hallar el **mejor modelo**."
      ],
      "metadata": {
        "id": "C087CQNbdH2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if df is not None:\n",
        "    # Calculate the correlation matrix\n",
        "    correlation_matrix = df.corr()\n",
        "\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\",\n",
        "                linewidths=.5)\n",
        "    plt.title('Feature Correlation Heatmap')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nJAUIbi6WkSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "División del conjunto de datos para **_entrenamiento, validación y test final_**."
      ],
      "metadata": {
        "id": "HmaK3G3Ae2BH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'X' in locals() and 'y' in locals():\n",
        "  \"\"\"\n",
        "   It is first divided into 2, and then one of those halves is subdivided into 2\n",
        "   to get the train/validation/testing structure.\n",
        "  \"\"\"\n",
        "  x_train, x_temp, y_train, y_temp = train_test_split(\n",
        "      X, y, test_size = 0.3, random_state=42,\n",
        "      stratify=y\n",
        "  )\n",
        "\n",
        "  x_val, x_test, y_val, y_test = train_test_split(\n",
        "      x_temp, y_temp, test_size=0.5, random_state=42,\n",
        "      stratify=y_temp\n",
        "  )\n",
        "\n",
        "  print(f\"Training set size: {x_train.shape[0]} samples\")\n",
        "  print(f\"Validation set size: {x_val.shape[0]} samples\")\n",
        "  print(f\"Test set size: {x_test.shape[0]} samples\")\n",
        "else:\n",
        "  print(\"Error: Variables X and y are not defined.\")"
      ],
      "metadata": {
        "id": "wV8l5dnedP5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Inicio de las diferentes pruebas**"
      ],
      "metadata": {
        "id": "KrKc3LyGArH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento del modelo con todas las features para poder verificar que su cargue, envío y construcción de hiperparámetros es correcto."
      ],
      "metadata": {
        "id": "hzhuxoble9iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'x_train' in locals():\n",
        "  \"\"\"\n",
        "    The model is trained with an accepted error of 0.0001 and a maximum of\n",
        "    10,000 iterations to avoid overfitting or divergence. In addition, seed 42\n",
        "    is used because it is widely deployed.\n",
        "  \"\"\"\n",
        "  print(\"Test with all features\")\n",
        "  logistic_model_1 = LogisticRegression(tol=0.0001, random_state=42, max_iter=10000)\n",
        "  logistic_model_1.fit(x_train, y_train)\n",
        "  y_pred_val_1 = logistic_model_1.predict(x_val)\n",
        "  accuracy_1 = accuracy_score(y_val, y_pred_val_1)\n",
        "  f1_1 = f1_score(y_val, y_pred_val_1)\n",
        "  print(f\"Accuracy: {accuracy_1:.5f}\")\n",
        "  print(f\"F1 Score: {f1_1:.5f}\")\n",
        "\n",
        "  cm_1 = confusion_matrix(y_val, y_pred_val_1)\n",
        "  sns.heatmap(cm_1, annot=True, fmt='d', cmap='Blues')\n",
        "  plt.title('Confusion Matrix - Test 1')\n",
        "  plt.xlabel('Prediction')\n",
        "  plt.ylabel('Actual Value')\n",
        "  plt.show()\n",
        "\n",
        "else:\n",
        "  print(\"Error: Variable x_train is not defined.\")\n"
      ],
      "metadata": {
        "id": "m7Hq0gBle5hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Prueba con Features de alta relación_"
      ],
      "metadata": {
        "id": "nK_J9HoIg7Nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'x_train' in locals():\n",
        "  \"\"\"\n",
        "    The same parameters are maintained, except that training is performed with\n",
        "    the best features according to the correlation analysis.\n",
        "  \"\"\"\n",
        "  selected_features =[\n",
        "      'cantidad_exclamaciones',\n",
        "      'cantidad_urls',\n",
        "      'javascript_embebido',\n",
        "      'adjuntos_ejecutables',\n",
        "      'adjuntos_sospechosos',\n",
        "      'lenguaje_imperativo'\n",
        "  ]\n",
        "\n",
        "  print(\"Test with some features\")\n",
        "  x_train_2 = x_train[selected_features]\n",
        "  x_val_2 = x_val[selected_features]\n",
        "\n",
        "  logistic_model_2 = LogisticRegression(tol=0.0001, random_state=42, max_iter=10000)\n",
        "  logistic_model_2.fit(x_train_2, y_train)\n",
        "\n",
        "  y_pred_val_2 = logistic_model_2.predict(x_val_2)\n",
        "  accuracy_2 = accuracy_score(y_val, y_pred_val_2)\n",
        "  f1_2 = f1_score(y_val, y_pred_val_2)\n",
        "\n",
        "  print(f\"Accuracy: {accuracy_2:.5f}\")\n",
        "  print(f\"F1 Score: {f1_2:.5f}\")\n",
        "\n",
        "  cm_2 = confusion_matrix(y_val, y_pred_val_2)\n",
        "  sns.heatmap(cm_2, annot=True, fmt='d', cmap='Blues')\n",
        "  plt.title('Confusion Matrix - Test 2')\n",
        "  plt.xlabel('Prediction')\n",
        "  plt.ylabel('Actual Value')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U3Qus_xIhBXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Prueba con eliminación de ruido en Features_"
      ],
      "metadata": {
        "id": "-fpkcSKqnHSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'x_train' in locals():\n",
        "  \"\"\"\n",
        "    Here, features with correlations below 0.1 are removed.\n",
        "  \"\"\"\n",
        "  excepted_features = [\n",
        "      'cantidad_interrogaciones',\n",
        "      'cantidad_dominios_urls',\n",
        "      'dominio_coincide'\n",
        "  ]\n",
        "  without_excepted_features = [col for col in X.columns if col not in excepted_features]\n",
        "  logistic_model_3 = LogisticRegression(tol=0.0001, random_state=42, max_iter=10000)\n",
        "  logistic_model_3.fit(x_train[without_excepted_features], y_train)\n",
        "  y_pred_val_3 = logistic_model_3.predict(x_val[without_excepted_features])\n",
        "  accuracy_3 = accuracy_score(y_val, y_pred_val_3)\n",
        "  f1_3 = f1_score(y_val, y_pred_val_3)\n",
        "  print(f\"Accuracy: {accuracy_3:.5f}\")\n",
        "  print(f\"F1 Score: {f1_3:.5f}\")\n",
        "\n",
        "  cm_3 = confusion_matrix(y_val, y_pred_val_3)\n",
        "  sns.heatmap(cm_3, annot=True, fmt='d', cmap='Blues')\n",
        "  plt.title('Confusion Matrix - Test 3')\n",
        "  plt.xlabel('Prediction')\n",
        "  plt.ylabel('Actual Value')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "KuDO3sOKrDUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Solo Features de correlación superior a 0.15_"
      ],
      "metadata": {
        "id": "EFCkS1jsjFwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'x_train' in locals():\n",
        "  \"\"\"\n",
        "    Here, on the contrary, only those greater than 0.15 are taken to validate\n",
        "    good correlations.\n",
        "  \"\"\"\n",
        "  features_over = [\n",
        "        'cantidad_urls',\n",
        "        'lenguaje_imperativo',\n",
        "        'adjuntos_sospechosos',\n",
        "        'javascript_embebido',\n",
        "        'adjuntos_ejecutables',\n",
        "        'cantidad_exclamaciones',\n",
        "        'cantidad_destinatarios',\n",
        "        'ip_en_url',\n",
        "        'idioma_diferente_usuario'\n",
        "  ]\n",
        "  logistic_model_4 = LogisticRegression(tol=0.0001, random_state=42, max_iter=1000)\n",
        "  logistic_model_4.fit(x_train[features_over], y_train)\n",
        "  y_pred_val_4 = logistic_model_4.predict(x_val[features_over])\n",
        "  accuracy_4 = accuracy_score(y_val, y_pred_val_4)\n",
        "  f1_4 = f1_score(y_val, y_pred_val_4)\n",
        "  print(f\"Accuracy: {accuracy_4:.5f}\")\n",
        "  print(f\"F1 Score: {f1_4:.5f}\")\n",
        "\n",
        "  cm_4 = confusion_matrix(y_val, y_pred_val_4)\n",
        "  sns.heatmap(cm_4, annot=True, fmt='d', cmap='Blues')\n",
        "  plt.title('Confusion Matrix - Test 4')\n",
        "  plt.xlabel('Prediction')\n",
        "  plt.ylabel('Actual Value')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "py-n_oanjLj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Normalización para repetir las pruebas\n",
        "Ahora se normalizan los datos para evitar aparición de outliers, y ver la diferencia con el entrenamiento sin la normalización."
      ],
      "metadata": {
        "id": "-bkKuvQRuDkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if 'x_train' in locals():\n",
        "  \"\"\"\n",
        "    The subdivisions of the dataset are standardized to improve the accuracy\n",
        "    of the models.\n",
        "  \"\"\"\n",
        "  scaler = StandardScaler()\n",
        "  x_train_scaled_ss = scaler.fit_transform(x_train)\n",
        "  x_val_scaled_ss = scaler.transform(x_val)\n",
        "  x_test_scaled_ss = scaler.transform(x_test)\n",
        "  x_train_scaled = pd.DataFrame(x_train_scaled_ss, columns=X.columns)\n",
        "  x_val_scaled = pd.DataFrame(x_val_scaled_ss, columns=X.columns)\n",
        "  x_test_scaled = pd.DataFrame(x_test_scaled_ss, columns=X.columns)\n",
        "\n",
        "  print(\"Data converted to DataFrame. The variables are ready for testing.\")"
      ],
      "metadata": {
        "id": "tNrfuI3UuXq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Pruebas con datos escalados\n",
        "\n",
        "Se realizan las mismas pruebas pero ahora con los datos normalizados, aquí ya no se realiza la matriz de confusión, ya que con el **_accuracy_** y el **_f1 score_** es suficiente para ver las principales características, y se elige el mejor para las **pruebas finales**."
      ],
      "metadata": {
        "id": "N5Y3IJGsxzTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  The process is the same as with the previous tests, only the standardized\n",
        "  subdivisions are used and the new results are obtained.\n",
        "\"\"\"\n",
        "\n",
        "# --- Test 1   ---\n",
        "print(\"Test 1 (All features) with scaled data\")\n",
        "logistic_model_1s = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logistic_model_1s.fit(x_train_scaled, y_train)\n",
        "y_pred_val_1s = logistic_model_1s.predict(x_val_scaled)\n",
        "print(f\"Accuracy: {accuracy_score(y_val, y_pred_val_1s):.5f}\")\n",
        "print(f\"F1 Score: {f1_score(y_val, y_pred_val_1s):.5f}\")\n",
        "\n",
        "# --- Test 2 ---\n",
        "selected_features =[\n",
        "      'cantidad_exclamaciones',\n",
        "      'cantidad_urls',\n",
        "      'javascript_embebido',\n",
        "      'adjuntos_ejecutables',\n",
        "      'adjuntos_sospechosos',\n",
        "      'lenguaje_imperativo'\n",
        "  ]\n",
        "print(f\"\\n Test 2 (High correlation) with scaled data\")\n",
        "logistic_model_2s = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logistic_model_2s.fit(x_train_scaled[selected_features], y_train)\n",
        "y_pred_val_2s = logistic_model_2s.predict(x_val_scaled[selected_features])\n",
        "print(f\"Accuracy {accuracy_score(y_val, y_pred_val_2s):.5f}\")\n",
        "print(f\"F1 Score {f1_score(y_val, y_pred_val_2s):.5f}\")\n",
        "\n",
        "# --- Test 3 ---\n",
        "excepted_features = [\n",
        "      'cantidad_interrogaciones',\n",
        "      'cantidad_dominios_urls',\n",
        "      'dominio_coincide'\n",
        "  ]\n",
        "features_3 = [col for col in X.columns if col not in excepted_features]\n",
        "print(f\"\\n Test 3 (Noise exclusion) with scaled data\")\n",
        "logistic_model_3s = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logistic_model_3s.fit(x_train_scaled[features_3], y_train)\n",
        "y_pred_val_3s = logistic_model_3s.predict(x_val_scaled[features_3])\n",
        "print(f\"Accuracy {accuracy_score(y_val, y_pred_val_3s):.5f}\")\n",
        "print(f\"F1 Score {f1_score(y_val, y_pred_val_3s):.5f}\")\n",
        "\n",
        "# --- Test 4 ---\n",
        "features_over = [\n",
        "        'cantidad_urls',\n",
        "        'lenguaje_imperativo',\n",
        "        'adjuntos_sospechosos',\n",
        "        'javascript_embebido',\n",
        "        'adjuntos_ejecutables',\n",
        "        'cantidad_exclamaciones',\n",
        "        'cantidad_destinatarios',\n",
        "        'ip_en_url',\n",
        "        'idioma_diferente_usuario'\n",
        "    ]\n",
        "print(f\"\\n Test 4 (Balanced) with scaled data \")\n",
        "logistic_model_4s = LogisticRegression(random_state=42, max_iter=1000)\n",
        "logistic_model_4s.fit(x_train_scaled[features_over], y_train)\n",
        "y_pred_val_4s = logistic_model_4s.predict(x_val_scaled[features_over])\n",
        "print(f\"Accuracy {accuracy_score(y_val, y_pred_val_4s):.5f}\")\n",
        "print(f\"F1 Score {f1_score(y_val, y_pred_val_4s):.5f}\")"
      ],
      "metadata": {
        "id": "Qej6kcBLx82l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prueba final\n",
        "A continuación, se presentan las correlaciones de la Features selectas para el mejor modelo, además, se realiza el testeo con el 15% del dataset.\n",
        "\n",
        "###Prueba con datos sin normalizar."
      ],
      "metadata": {
        "id": "R8_7iD2LjXn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "  The best trained model is taken, the correlation of its features is sought,\n",
        "  and then it is tested with the last subdivision, which is the testing,\n",
        "  and its true accuracy is found.\n",
        "\"\"\"\n",
        "features_del_mejor_modelo = features_3\n",
        "mejor_modelo = logistic_model_3\n",
        "\n",
        "features_para_mapa = features_del_mejor_modelo + ['clase']\n",
        "\n",
        "correlation_matrix_optima = df[features_para_mapa].corr()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix_optima, annot=True, cmap='viridis', fmt=\".2f\")\n",
        "plt.title('Mapa de Correlación de la Función Óptima')\n",
        "plt.show()\n",
        "\n",
        "x_test_final = x_test[features_del_mejor_modelo]\n",
        "y_pred_final = mejor_modelo.predict(x_test_final)\n",
        "print(\"--- RESULTADOS FINALES DEL MODELO ÓPTIMO EN EL CONJUNTO DE PRUEBA ---\")\n",
        "final_accuracy = accuracy_score(y_test, y_pred_final)\n",
        "final_f1 = f1_score(y_test, y_pred_final)\n",
        "print(f\"\\nExactitud Final: {final_accuracy:.5f}\")\n",
        "print(f\"Puntuación F1 Final: {final_f1:.5f}\")\n",
        "\n",
        "print(\"\\n--- Reporte de Clasificación Detallado ---\")\n",
        "print(classification_report(y_test, y_pred_final, target_names=['HAM (Clase 0)', 'SPAM (Clase 1)']))\n",
        "\n",
        "print(\"\\n--- Matriz de Confusión Final ---\")\n",
        "cm_final = confusion_matrix(y_test, y_pred_final)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_final, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=['Predicción HAM', 'Predicción SPAM'],\n",
        "            yticklabels=['Real HAM', 'Real SPAM'])\n",
        "plt.title('Matriz de Confusión del Conjunto de Prueba')\n",
        "plt.ylabel('Etiqueta Real')\n",
        "plt.xlabel('Etiqueta Predicha')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HCHRFlCcjdNZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prueba con datos normalizados\n",
        "\n"
      ],
      "metadata": {
        "id": "4Ab_XcFF4Zi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  The test is repeated but with normalized data in order to make a better\n",
        "  assessment.\n",
        "\"\"\"\n",
        "features_del_mejor_modelo = features_3\n",
        "mejor_modelo = logistic_model_3s\n",
        "\n",
        "features_para_mapa = features_del_mejor_modelo + ['clase']\n",
        "\n",
        "correlation_matrix_optima = df[features_para_mapa].corr()\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix_optima, annot=True, cmap='viridis', fmt=\".2f\")\n",
        "plt.title('Mapa de Correlación de la Función Óptima')\n",
        "plt.show()\n",
        "\n",
        "x_test_final = x_test_scaled[features_del_mejor_modelo]\n",
        "y_pred_final = mejor_modelo.predict(x_test_final)\n",
        "print(\"--- RESULTADOS FINALES DEL MODELO ÓPTIMO EN EL CONJUNTO DE PRUEBA ---\")\n",
        "final_accuracy = accuracy_score(y_test, y_pred_final)\n",
        "final_f1 = f1_score(y_test, y_pred_final)\n",
        "print(f\"\\nExactitud Final: {final_accuracy:.5f}\")\n",
        "print(f\"Puntuación F1 Final: {final_f1:.5f}\")\n",
        "\n",
        "print(\"\\n--- Reporte de Clasificación Detallado ---\")\n",
        "print(classification_report(y_test, y_pred_final, target_names=['HAM (Clase 0)', 'SPAM (Clase 1)']))\n",
        "\n",
        "print(\"\\n--- Matriz de Confusión Final ---\")\n",
        "cm_final = confusion_matrix(y_test, y_pred_final)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_final, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=['Predicción HAM', 'Predicción SPAM'],\n",
        "            yticklabels=['Real HAM', 'Real SPAM'])\n",
        "plt.title('Matriz de Confusión del Conjunto de Prueba')\n",
        "plt.ylabel('Etiqueta Real')\n",
        "plt.xlabel('Etiqueta Predicha')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uwvpRMGl4fl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Calculo de outliers\n",
        "\n",
        "Finalmente se puede buscar cuales fueron los errores por los cuales el modelo estaba seguro erroneamente, y calcular su distancia en un punto dado, lo cual seria su error."
      ],
      "metadata": {
        "id": "F6uHaXcsjlqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "  The linear function resulting from the best model is analyzed, the distances\n",
        "  are calculated, and those values that deviate from this mean are taken as error\n",
        " and their difference is calculated.\n",
        "\"\"\"\n",
        "features_del_mejor_modelo = features_3\n",
        "mejor_modelo = logistic_model_3s\n",
        "\n",
        "y_pred_val_optimo = mejor_modelo.predict(x_val_scaled[features_del_mejor_modelo])\n",
        "distancia_optima = mejor_modelo.decision_function(x_val_scaled[features_del_mejor_modelo])\n",
        "\n",
        "# Creamos un DataFrame para analizar los errores\n",
        "analisis_df_optimo = pd.DataFrame({\n",
        "    'actual_value': y_val.values,\n",
        "    'prediction': y_pred_val_optimo,\n",
        "    'decision distance': distancia_optima\n",
        "})\n",
        "\n",
        "# Filtramos para quedarnos solo con los errores\n",
        "errores_df_optimo = analisis_df_optimo[analisis_df_optimo['actual_value'] != analisis_df_optimo['prediction']].copy()\n",
        "errores_df_optimo['confianza_error'] = np.abs(errores_df_optimo['decision distance'])\n",
        "errores_df_optimo = errores_df_optimo.sort_values(by='confianza_error', ascending=False)\n",
        "\n",
        "print(\"\\n Analysis of outliers in the optimal model \")\n",
        "print(\"The errors in which the model was most ‘certain’ of its incorrect prediction:\")\n",
        "print(errores_df_optimo.head())"
      ],
      "metadata": {
        "id": "KKdSuoNbjqeH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}